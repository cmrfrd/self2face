{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "By: Alex Comerford (alexanderjcomerford@gmail.com)\n",
    "\n",
    "In this notebook we will be preparing and organizing data for our machine learning model. This model is based off of neural networks, specifically using the Generative Adversarial Network architecture. The data we will be providing to this network will be image based data extracted from a video.\n",
    "\n",
    "Our input format is `webm` and our desired output format is `png`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment setup\n",
    "\n",
    "First we will install some high level dependencies for our data extraction process. In this case we will be using the `ffmpeg` library to extract images from our `webm` input format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies\n",
    "\n",
    "Now that we have our environment setup we will be importing all the dependencies we will need to extract images from our videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dlib\n",
    "import ffmpeg\n",
    "import shutil\n",
    "import PIL.Image\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramaters\n",
    "\n",
    "In this notebook we will be defining the high level parameters to be used throughout the rest of the notebook. In this case we only need to define 3. The input webm file, the output save path, and the number of images to produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE                     = \"./data/raw/2018-12-02-215035.webm\"\n",
    "ORIGINAL_SAVE_PATH       = \"./data/original_images/\"\n",
    "LANDMARK_SAVE_PATH       = \"./data/landmark_images/\"\n",
    "FACE_LANDMARK_SHAPE_FILE = \"./models/shape_predictor_68_face_landmarks.dat\"\n",
    "MAX_NUM_IMAGES           = 5000\n",
    "DOWNSAMPLE_RATIO         = 2\n",
    "input_filetype           = \".jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the input file\n",
    "\n",
    "In this next cell we will get some high level informatino about our input video including the number of total frames in the video, location, and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Location =  ./data/raw/2018-12-02-215035.webm\n",
      "File Size     =  53890813\n",
      "Num Frames    =  11743\n",
      "Width         =  640\n",
      "Height        =  480\n"
     ]
    }
   ],
   "source": [
    "## Use ffmpeg to get the number of frames\n",
    "def get_num_frames(input_file):\n",
    "    frames = !ffmpeg -i {FILE} -map 0:v:0 -c copy -f null -y /dev/null 2>&1 | grep -Eo 'frame= *[0-9]+ *' | grep -Eo '[0-9]+' | tail -1\n",
    "    frames = int(frames[0])\n",
    "    return frames\n",
    "\n",
    "## Extract informatino about input file\n",
    "probe = ffmpeg.probe(FILE)\n",
    "video = ffmpeg.input(FILE)\n",
    "video_info = next(s for s in probe['streams'] if s['codec_type'] == 'video')\n",
    "width = int(video_info['width'])\n",
    "height = int(video_info['height'])\n",
    "num_frames = get_num_frames(FILE)\n",
    "\n",
    "print (\"File Location = \", FILE)\n",
    "print (\"File Size     = \", os.path.getsize(FILE))\n",
    "print (\"Num Frames    = \", num_frames)\n",
    "print (\"Width         = \", width)\n",
    "print (\"Height        = \", height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_frame(video_input, n):\n",
    "    '''get_n_frame\n",
    "    \n",
    "    Given an ffmpeg video input source, return the\n",
    "    nth frame as a numpy array\n",
    "    '''\n",
    "\n",
    "    out, _ = (\n",
    "        video_input.filter('select', 'gte(n,{})'.format(n))\n",
    "        .output('pipe:', format='rawvideo', pix_fmt='rgb24', vframes=1)\n",
    "        .run(capture_stdout=True, capture_stderr=True)\n",
    "    )\n",
    "    extracted_frame = np.frombuffer(out, np.uint8).reshape([height, width, 3])\n",
    "    return extracted_frame\n",
    "\n",
    "def save_n_frame(video_input, save_path, n):\n",
    "    '''save_n_frame\n",
    "    \n",
    "    Given an ffmpeg video input source save, the nth\n",
    "    frame as a jpg\n",
    "    '''\n",
    "    video_input.filter('select', 'gte(n,{})'.format(n))\\\n",
    "               .output(os.path.join(save_path,'%d.jpg'%n), \n",
    "                       vframes=1, \n",
    "                       format='image2', \n",
    "                       vcodec='mjpeg')\\\n",
    "               .overwrite_output()\\\n",
    "               .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(input_frame):\n",
    "    def reshape_for_polyline(array):\n",
    "        return np.array(array, np.int32).reshape((-1, 1, 2))\n",
    "    \n",
    "    frame_resize = cv2.resize(input_frame, None, fx=1 / DOWNSAMPLE_RATIO, fy=1 / DOWNSAMPLE_RATIO)\n",
    "    gray = cv2.cvtColor(frame_resize, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray, 1)\n",
    "    black_image = np.zeros(input_frame.shape, np.uint8)\n",
    "\n",
    "    # Perform if there is a face detected\n",
    "    if len(faces) >= 1:\n",
    "        for face in faces:\n",
    "            detected_landmarks = predictor(gray, face).parts()\n",
    "            landmarks = [[p.x * DOWNSAMPLE_RATIO, p.y * DOWNSAMPLE_RATIO] for p in detected_landmarks]\n",
    "\n",
    "            jaw = reshape_for_polyline(landmarks[0:17])\n",
    "            left_eyebrow = reshape_for_polyline(landmarks[22:27])\n",
    "            right_eyebrow = reshape_for_polyline(landmarks[17:22])\n",
    "            nose_bridge = reshape_for_polyline(landmarks[27:31])\n",
    "            lower_nose = reshape_for_polyline(landmarks[30:35])\n",
    "            left_eye = reshape_for_polyline(landmarks[42:48])\n",
    "            right_eye = reshape_for_polyline(landmarks[36:42])\n",
    "            outer_lip = reshape_for_polyline(landmarks[48:60])\n",
    "            inner_lip = reshape_for_polyline(landmarks[60:68])\n",
    "\n",
    "            color = (255, 255, 255)\n",
    "            thickness = 3\n",
    "\n",
    "            cv2.polylines(black_image, [jaw], False, color, thickness)\n",
    "            cv2.polylines(black_image, [left_eyebrow], False, color, thickness)\n",
    "            cv2.polylines(black_image, [right_eyebrow], False, color, thickness)\n",
    "            cv2.polylines(black_image, [nose_bridge], False, color, thickness)\n",
    "            cv2.polylines(black_image, [lower_nose], True, color, thickness)\n",
    "            cv2.polylines(black_image, [left_eye], True, color, thickness)\n",
    "            cv2.polylines(black_image, [right_eye], True, color, thickness)\n",
    "            cv2.polylines(black_image, [outer_lip], True, color, thickness)\n",
    "            cv2.polylines(black_image, [inner_lip], True, color, thickness)\n",
    "            \n",
    "        return input_frame, black_image\n",
    "    return (False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting frames/images\n",
    "\n",
    "In the next cell we will be using ffmpeg in python to extract MAX_NUM_IMAGES of images into our output directory. Afterwards we will rearchetest the entire directory structure in a tree format. This seems weird but is easier to batch files in sub directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(input_file, \n",
    "                    original_save_path, \n",
    "                    landmark_save_path,\n",
    "                    max_num_images,\n",
    "                    log_every=100):\n",
    "    \n",
    "    ## make dir if doesn't exist\n",
    "    os.makedirs(original_save_path, exist_ok=True)\n",
    "    os.makedirs(landmark_save_path, exist_ok=True)\n",
    "    \n",
    "    ## Func to count files in dir\n",
    "    num_files_in_dir = lambda path: sum(list(len(f) for _,_,f in os.walk(path)))\n",
    "    \n",
    "    ## Divisor for number of frames seperated between iterations\n",
    "    divisor = 1 if not int(num_frames / max_num_images) \\\n",
    "                else int(num_frames / max_num_images)\n",
    "    \n",
    "    ## Check if dataset already exists\n",
    "    if (num_files_in_dir(original_save_path) >= (num_frames / divisor)) and \\\n",
    "       (num_files_in_dir(landmark_save_path) >= (num_frames / divisor)):\n",
    "        print (\"Dataset already created, returning ...\")\n",
    "        return\n",
    "    \n",
    "    successful_extractions = 0\n",
    "    failed_extractions = 0\n",
    "    for i in range(0,num_frames,divisor):\n",
    "\n",
    "            ## Log images\n",
    "            if i%log_every==0:\n",
    "                print (\"--------------------------\")\n",
    "                print (\"%d iterations...\"%i)\n",
    "                print (\"%d successful_extractions\"%successful_extractions)\n",
    "                print (\"%d failed_extractions\"%failed_extractions)\n",
    "\n",
    "            ## make numeric group directories\n",
    "            original_save_path_group = os.path.join(original_save_path, '%s'%str(i)[0])\n",
    "            landmark_save_path_group = os.path.join(landmark_save_path, '%s'%str(i)[0])\n",
    "            os.makedirs(original_save_path_group, exist_ok=True)\n",
    "            os.makedirs(landmark_save_path_group, exist_ok=True)\n",
    "            \n",
    "            if len(str(i)) > 1:\n",
    "                original_save_path_group = os.path.join(original_save_path_group, '%s'%str(i)[1])\n",
    "                landmark_save_path_group = os.path.join(landmark_save_path_group, '%s'%str(i)[1])\n",
    "                os.makedirs(original_save_path_group,exist_ok=True)\n",
    "                os.makedirs(landmark_save_path_group,exist_ok=True)            \n",
    "            \n",
    "            ## extract landmarks and frame\n",
    "            extracted_frame = get_n_frame(video, i)\n",
    "            extracted_frame, extracted_landmark = extract_landmarks(extracted_frame)\n",
    "            \n",
    "            if type(extracted_frame) == bool:\n",
    "                failed_extractions += 1\n",
    "            else:\n",
    "                \n",
    "                try:\n",
    "                    save_n_frame(video, original_save_path_group, i)\n",
    "                    cv2.imwrite(os.path.join(landmark_save_path_group, \"%d.jpg\"%i), \n",
    "                                extracted_landmark)\n",
    "                except Exception as e:\n",
    "                    if os.path.exists(os.path.join(landmark_save_path_group, \"%d.jpg\"%i)) and \\\n",
    "                       os.path.exists(os.path.join(original_save_path_group,'%d.jpg'%i)):\n",
    "                        successful_extractions += 1\n",
    "                    else:\n",
    "                        failed_extractions += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "0 iterations...\n",
      "0 successful_extractions\n",
      "0 failed_extractions\n",
      "--------------------------\n",
      "100 iterations...\n",
      "1 successful_extractions\n",
      "45 failed_extractions\n",
      "--------------------------\n",
      "200 iterations...\n",
      "1 successful_extractions\n",
      "95 failed_extractions\n"
     ]
    }
   ],
   "source": [
    "# Create the face predictor and landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(FACE_LANDMARK_SHAPE_FILE)\n",
    "\n",
    "prepare_dataset(FILE, \n",
    "                ORIGINAL_SAVE_PATH, \n",
    "                LANDMARK_SAVE_PATH,\n",
    "                MAX_NUM_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "\"self2face-gpu\"",
   "language": "python",
   "name": "self2face-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
