{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Models\n",
    "\n",
    "In order to make a tensorflow model usable by other systems, tensorflow provides multiple different methods to export a model. For this use case we will be defining a subgraph of the original model.\n",
    "\n",
    "We will then produce a final \"frozen\" graph which is an optimized version of an original model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing\n",
    "\n",
    "Below we import all out utilities as well as cells from the build and train notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_util\n",
    "\n",
    "import tensorflow as tf;tf.reset_default_graph()\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "import math\n",
    "import time\n",
    "\n",
    "from util import run_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cells(\"./BuildingAndTrainModels.ipynb\", \n",
    "          cell_tags=[\"parameters\",\"conv\",\"gen\",\"disc\",\"model\",\"util\",\"process\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing the model\n",
    "\n",
    "Before we can export the model as a single file, we must reduce the model into a smaller subgraph containing just the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_generator():\n",
    "\n",
    "    ## Create model inputs\n",
    "    input = tf.placeholder(tf.string, shape=[1])\n",
    "    input_image = tf.image.decode_png(tf.decode_base64(input[0]))\n",
    "\n",
    "    # remove alpha channel if present\n",
    "    input_image = tf.cond(tf.equal(tf.shape(input_image)[2], 4), lambda: input_image[:,:,:3], lambda: input_image)\n",
    "    # convert grayscale to RGB\n",
    "    input_image = tf.cond(tf.equal(tf.shape(input_image)[2], 1), lambda: tf.image.grayscale_to_rgb(input_image), lambda: input_image)\n",
    "\n",
    "    ## Crop and batch input\n",
    "    input_image = tf.image.convert_image_dtype(input_image, dtype=tf.float32)\n",
    "    input_image.set_shape([CROP_SIZE, CROP_SIZE, 3])\n",
    "    batch_input = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "    ## Create generator model\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        batch_output = deprocess(create_generator(preprocess(batch_input), 3))\n",
    "\n",
    "    ## Convert model output to filetype output\n",
    "    output_filetype = \"png\"\n",
    "    output_image = tf.image.convert_image_dtype(batch_output, dtype=tf.uint8)[0]\n",
    "    if output_filetype == \"png\":\n",
    "        output_data = tf.image.encode_png(output_image)\n",
    "    elif output_filetype == \"jpeg\":\n",
    "        output_data = tf.image.encode_jpeg(output_image, quality=80)\n",
    "    else:\n",
    "        raise Exception(\"invalid filetype\")\n",
    "    output = tf.convert_to_tensor([tf.encode_base64(output_data)])\n",
    "    \n",
    "    return input, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a helper function we can produce our inputs and outputs for this respective model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, output = create_single_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the input and output tensors to the graph collection that tensorflow is aware of\n",
    "\n",
    "We will then use this collected ops in the graph to load our previously saved model, then after loading it, we will newly export it to a file containing only the weights we care about transfered to this new subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = tf.placeholder(tf.string, shape=[1])\n",
    "inputs = {\n",
    "    \"key\": key.name,\n",
    "    \"input\": input.name\n",
    "}\n",
    "tf.add_to_collection(\"inputs\", json.dumps(inputs))\n",
    "outputs = {\n",
    "    \"key\":  tf.identity(key).name,\n",
    "    \"output\": output.name,\n",
    "}\n",
    "tf.add_to_collection(\"outputs\", json.dumps(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing and Exporting\n",
    "\n",
    "In the next cell we will read in a tensorflow checkpoint, then exporting the graph to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint ...\n",
      "INFO:tensorflow:Restoring parameters from ./models/face2face-model/model-32000\n",
      "Exporting model ...\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "restore_saver = tf.train.Saver()\n",
    "export_saver = tf.train.Saver()\n",
    "\n",
    "input_checkpoint_dir = \"./models/face2face-model/\"\n",
    "output_model_dir = \"./models/face2face-reduced-model/\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    print(\"Loading model from checkpoint ...\")\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(input_checkpoint_dir)\n",
    "    restore_saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    print(\"Exporting model ...\")\n",
    "    export_saver.export_meta_graph(filename=os.path.join(output_model_dir, \"export.meta\"))\n",
    "    export_saver.save(sess, os.path.join(output_model_dir, \"export\"), write_meta_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/face2face-model/export\n",
      "INFO:tensorflow:Restoring parameters from ./models/face2face-model/export\n",
      "Model is exported to ./models/face2face-model/export\n"
     ]
    }
   ],
   "source": [
    "def process_image(x):\n",
    "    with tf.name_scope('load_images'):\n",
    "        raw_input = tf.image.convert_image_dtype(x, dtype=tf.float32)\n",
    "\n",
    "        raw_input.set_shape([None, None, 3])\n",
    "\n",
    "        # break apart image pair and move to range [-1, 1]\n",
    "        width = tf.shape(raw_input)[1]  # [height, width, channels]\n",
    "        a_images = preprocess(raw_input[:, :width // 2, :])\n",
    "        b_images = preprocess(raw_input[:, width // 2:, :])\n",
    "\n",
    "    inputs, targets = [a_images, b_images]\n",
    "\n",
    "    # synchronize seed for image operations so that we do the same operations to both\n",
    "    # input and output images\n",
    "    def transform(image):\n",
    "        # area produces a nice downscaling, but does nearest neighbor for upscaling\n",
    "        # assume we're going to be doing downscaling here\n",
    "        return tf.image.resize_images(image, [CROP_SIZE, CROP_SIZE], method=tf.image.ResizeMethod.AREA)\n",
    "\n",
    "    with tf.name_scope('input_images'):\n",
    "        input_images = tf.expand_dims(transform(inputs), 0)\n",
    "\n",
    "    with tf.name_scope('target_images'):\n",
    "        target_images = tf.expand_dims(transform(targets), 0)\n",
    "\n",
    "    return input_images, target_images\n",
    "\n",
    "def create_model(inputs, targets):\n",
    "    with tf.variable_scope('generator') as scope:\n",
    "        out_channels = int(targets.get_shape()[-1])\n",
    "        outputs = create_generator(inputs, out_channels)\n",
    "    return outputs\n",
    "\n",
    "def convert(image):\n",
    "    return tf.image.convert_image_dtype(image, dtype=tf.uint8, saturate=True, name='output')  # output tensor\n",
    "\n",
    "def generate_output(x):\n",
    "    with tf.name_scope('generate_output'):\n",
    "        test_inputs, test_targets = process_image(x)\n",
    "\n",
    "        # inputs and targets are [batch_size, height, width, channels]\n",
    "        model = create_model(test_inputs, test_targets)\n",
    "\n",
    "        # deprocess files\n",
    "        outputs = deprocess(model)\n",
    "\n",
    "        # reverse any processing on images so they can be written to disk or displayed to user\n",
    "        converted_outputs = convert(outputs)\n",
    "    return converted_outputs\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_folder = \"./models/face2face-model/\"\n",
    "output_folder = \"./models/face2face-reduced-model/\"\n",
    "\n",
    "a = tf.placeholder(tf.uint8, shape=(256, 512, 3), name='image_tensor')  # input tensor\n",
    "y = generate_output(a)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore original model\n",
    "    saver = tf.train.Saver()\n",
    "    checkpoint = tf.train.latest_checkpoint(input_folder)\n",
    "    print (checkpoint)\n",
    "    saver.restore(sess, checkpoint)\n",
    "\n",
    "    # Export reduced model used for prediction\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, '{}/reduced_model'.format(output_folder))\n",
    "    print(\"Model is exported to {}\".format(checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freezing the model\n",
    "\n",
    "Now that we have a reduced form of our model we will now \"freeze\" it by exporting the tensorflow checkpoint to a frozen protobuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retrieve our checkpoint fullpath\n",
    "checkpoint = tf.train.get_checkpoint_state(output_folder)\n",
    "input_checkpoint = checkpoint.model_checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We precise the file fullname of our freezed graph\n",
    "absolute_model_folder = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "output_graph = output_folder + '/frozen_model.pb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before exporting our graph, we need to precise what is our output node\n",
    "# This is how TF decides what part of the Graph he has to keep and what part it can dump\n",
    "# NOTE: this variable is plural, because you can have multiple output nodes\n",
    "output_node_names = 'generate_output/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We clear devices to allow TensorFlow to control on which device it will load operations\n",
    "clear_devices = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the meta graph and retrieve a Saver\n",
    "saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/face2face-reduced-model/reduced_model\n",
      "INFO:tensorflow:Froze 76 variables.\n",
      "Converted 76 variables to const ops.\n",
      "405 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "# We retrieve the protobuf graph definition\n",
    "graph = tf.get_default_graph()\n",
    "input_graph_def = graph.as_graph_def()\n",
    "\n",
    "# We start a session and restore the graph weights\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, input_checkpoint)\n",
    "\n",
    "    # We use a built-in TF helper to export variables to constants\n",
    "    output_graph_def = graph_util.convert_variables_to_constants(\n",
    "        sess,  # The session is used to retrieve the weights\n",
    "        input_graph_def,  # The graph_def is used to retrieve the nodes\n",
    "        output_node_names.split(\",\")  # The output node names are used to select the usefull nodes\n",
    "    )\n",
    "\n",
    "    # Finally we serialize and dump the output graph to the filesystem\n",
    "    with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "    print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
