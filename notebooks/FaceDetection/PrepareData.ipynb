{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "In this notebook we will be generically preparing data... from minio!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_SERVICE='minio.default.svc.cluster.local:9000'\n",
    "MINIO_ACCESS_KEY='self2face'\n",
    "MINIO_SECRET_KEY='self2face'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AWS_ACCESS_KEY_ID=self2face\n",
      "env: AWS_SECRET_ACCESS_KEY=self2face\n",
      "env: S3_ENDPOINT=minio.default.svc.cluster.local:9000\n",
      "env: AWS_ENDPOINT_URL=http://minio.default.svc.cluster.local:9000\n",
      "env: S3_USE_HTTPS=0\n",
      "env: S3_VERIFY_SSL=0\n",
      "env: BUCKET_NAME=test-data\n"
     ]
    }
   ],
   "source": [
    "get_ipython().run_line_magic(\"env\", \"AWS_ACCESS_KEY_ID=self2face\")\n",
    "get_ipython().run_line_magic(\"env\", \"AWS_SECRET_ACCESS_KEY=self2face\")\n",
    "get_ipython().run_line_magic(\"env\", \"S3_ENDPOINT=minio.default.svc.cluster.local:9000\")\n",
    "get_ipython().run_line_magic(\"env\", \"AWS_ENDPOINT_URL=http://minio.default.svc.cluster.local:9000\")\n",
    "get_ipython().run_line_magic(\"env\", \"S3_USE_HTTPS=0\")\n",
    "get_ipython().run_line_magic(\"env\", \"S3_VERIFY_SSL=0\")\n",
    "get_ipython().run_line_magic(\"env\", \"BUCKET_NAME=test-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import glob\n",
    "import enum\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from minio import Minio\n",
    "from minio.error import ResponseError\n",
    "import tensorflow_datasets.public_api as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# servicename.namespace.svc.cluster.local\n",
    "# minio_client = Minio(os.environ[\"S3_ENDPOINT\"],\n",
    "#                      access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "#                      secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "#                      secure=False)\n",
    "\n",
    "# buckets = minio_client.list_buckets()\n",
    "\n",
    "# for bucket in buckets:\n",
    "#     print(bucket.name, bucket.creation_date)\n",
    "#     print(list(map(get_name, minio_client.list_objects_v2(\"test-data\", '/', recursive=True))))\n",
    "\n",
    "# minio_client.bucket_exists(\"test-data\")\n",
    "\n",
    "# get_name = lambda object: object.object_name\n",
    "# names = map(get_name, client.list_objects_v2(\"test-data\", '/', recursive=True))\n",
    "# for err in client.remove_objects(\"test-data\", names):\n",
    "#     print(\"Deletion Error: {}\".format(err))\n",
    "# client.remove_bucket(\"test-data\")\n",
    "\n",
    "# client.make_bucket(\"test-data\")\n",
    "\n",
    "# client.fput_object('test-data','asdf.jpg','./2002/08/11/big/img_591.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_string_line = lambda line:str(line.decode('utf-8').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_from_tar(filepath, tarpath):\n",
    "    '''Given a tarpath extract a file from tar'''\n",
    "    return tarfile.open(tarpath).extractfile(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_image(fileobj):\n",
    "    '''Given a file obj, attempt to create an Image'''\n",
    "    nparr = np.frombuffer(fileobj.read(), np.uint8)\n",
    "    img_np = cv2.cvtColor(cv2.imdecode(nparr, 1), cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(img_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_dir(input_dir):\n",
    "    return max(glob.glob(os.path.join(input_dir, '*/')), key=os.path.getmtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_download_path(url, base_dir=\"./data\"):\n",
    "    base_download_filename = os.path.basename(url)\n",
    "    for (dirpath, dirnames, filenames) in os.walk(base_dir):\n",
    "        if base_download_filename in filenames:\n",
    "            downloaded_filepath = os.path.join(dirpath, filenames[0])\n",
    "            break\n",
    "    return downloaded_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDDB specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_ellipse_to_box(image, major_axis_radius, minor_axis_radius, angle, center_x, center_y, detection_score):\n",
    "    '''Given a PIL image and ellipse information, return a dict with bounding box information'''\n",
    "    \n",
    "    imagew=image.size[1]\n",
    "    imageh=image.size[0]\n",
    "    box = dotdict(\n",
    "        x=1.0*center_x/imagew,\n",
    "        y=1.0*center_y/imageh,\n",
    "        w=1.0*minor_axis_radius*2/imagew,\n",
    "        h=1.0*major_axis_radius*2/imageh,\n",
    "        category=0\n",
    "    )\n",
    "    \n",
    "    if box.w>0 and box.h>0 and box.x-box.w/2>=0 and\\\n",
    "       box.y-box.h/2>=0 and box.x+box.w/2<=1 and box.y+box.h/2<=1:\n",
    "        return box\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FDDB_Dataset(tfds.core.GeneratorBasedBuilder):\n",
    "    \"\"\"Short description of my dataset.\"\"\"\n",
    "    \n",
    "    class FDDB_Parse_State(enum.Enum):\n",
    "        '''States of fddb dataset annotations'''\n",
    "        FILEPATH = 1\n",
    "        NUMFACES = 2\n",
    "        FACELOCATION = 3\n",
    "        \n",
    "    FDDB_BUCKET_NAME = \"fddb\"\n",
    "        \n",
    "    FDDB_DOWNLOAD_URLS={\n",
    "        \"images\":\"http://tamaraberg.com/faceDataset/originalPics.tar.gz\",\n",
    "        \"annotations\":\"http://vis-www.cs.umass.edu/fddb/FDDB-folds.tgz\"\n",
    "    }\n",
    "\n",
    "    VERSION = tfds.core.Version('0.0.0')\n",
    "    \n",
    "    def __init__(self, download_local, *args, **kwargs):\n",
    "        super(FDDB_Dataset, self).__init__(*args, *kwargs)\n",
    "        self.download_local = download_local\n",
    "        self.minio_client = self.make_minio_client()\n",
    "        self.data_count = 0\n",
    "\n",
    "    def _info(self):\n",
    "        return tfds.core.DatasetInfo(\n",
    "            builder=self,\n",
    "            features=tfds.features.FeaturesDict({\n",
    "                \"image\": tfds.features.Image(),\n",
    "                \"bbox\": tfds.features.BBoxFeature()\n",
    "            }),\n",
    "            supervised_keys=(\"image\", \"bbox\")\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        try:\n",
    "            dl_paths = dl_manager.download(self.FDDB_DOWNLOAD_URLS)\n",
    "        except tfds.download.download_manager.NonMatchingChecksumError:\n",
    "            pass\n",
    "        \n",
    "        return self.extract_and_upload()\n",
    "        \n",
    "    def _generate_examples(self):\n",
    "        # Yields examples from the dataset\n",
    "        pass  # TODO        \n",
    "\n",
    "    def make_minio_client(self, **kwargs):\n",
    "        return Minio(os.environ[\"S3_ENDPOINT\"],\n",
    "                     access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "                     secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "                     secure=False,\n",
    "                     **kwargs)\n",
    "    \n",
    "    def upload_file_to_minio(self, bucket_name, objname, fileobj, size):\n",
    "        '''Wrapper for uploading a file opject to minio'''\n",
    "        try:\n",
    "            if not self.minio_client.bucket_exists(bucket_name):\n",
    "                self.minio_client.make_bucket(bucket_name)\n",
    "        except ResponseError:\n",
    "            pass        \n",
    "        \n",
    "        return self.minio_client.put_object(bucket_name, objname, fileobj, size)\n",
    "    \n",
    "    def generate_fddb_json_annotations(self, annotations_file_content, images_tarfile, image_file_extension=\".jpg\"):\n",
    "        '''Generator of json annotations for fddb dataset'''\n",
    "\n",
    "        ## Define base dict, state variable, and first line\n",
    "        base_json_annotation = dotdict()\n",
    "        current_state = self.FDDB_Parse_State.FILEPATH\n",
    "        line = normalize_string_line(annotations_file_content.readline())\n",
    "\n",
    "        ## Iter line until empty file\n",
    "        while line:\n",
    "\n",
    "            if current_state == self.FDDB_Parse_State.FILEPATH:\n",
    "                image = file_to_image(images_tarfile.extractfile(line + image_file_extension))\n",
    "                base_json_annotation['file'] = line + image_file_extension\n",
    "                base_json_annotation['imagew'] = image.size[1]\n",
    "                base_json_annotation['imageh'] = image.size[0]            \n",
    "                current_state = self.FDDB_Parse_State.NUMFACES\n",
    "\n",
    "            elif current_state == self.FDDB_Parse_State.NUMFACES:\n",
    "                face_locations = []\n",
    "                num_faces = int(line)\n",
    "                current_state = self.FDDB_Parse_State.FACELOCATION\n",
    "\n",
    "            elif current_state == self.FDDB_Parse_State.FACELOCATION:\n",
    "                if num_faces > 0:\n",
    "                    face_location_args = map(float,line.split())\n",
    "                    bbox = image_ellipse_to_box(image, *face_location_args)\n",
    "                    if bbox: face_locations.append(bbox)\n",
    "                    num_faces -= 1\n",
    "                else:\n",
    "                    if len(face_locations):\n",
    "                        yield dotdict({\n",
    "                            **base_json_annotation, \n",
    "                            **{\"face_locations\":face_locations}})                \n",
    "                    current_state = self.FDDB_Parse_State.FILEPATH\n",
    "                    continue\n",
    "\n",
    "            line = normalize_string_line(annotations_file_content.readline())\n",
    "            \n",
    "    def extract_and_upload(self):\n",
    "\n",
    "        images_tarfile_path = search_download_path(self.FDDB_DOWNLOAD_URLS[\"images\"])\n",
    "        images_tarfile = tarfile.open(images_tarfile_path)\n",
    "        annotations_tarfile_path = search_download_path(self.FDDB_DOWNLOAD_URLS[\"annotations\"])\n",
    "        annotations_tarfile = tarfile.open(annotations_tarfile_path)\n",
    "        \n",
    "        for annotation_file in filter(lambda tfn:\"ellipse\" in tfn.name, annotations_tarfile):\n",
    "            for fddb_json_annotations in self.generate_fddb_json_annotations(annotations_tarfile.extractfile(annotation_file), images_tarfile):\n",
    "                if not self.download_local:\n",
    "                    image_response = self.upload_file_to_minio(bucket_name = self.FDDB_BUCKET_NAME, \n",
    "                                                               objname = fddb_json_annotations[\"file\"],\n",
    "                                                               fileobj = get_file_from_tar(fddb_json_annotations[\"file\"], images_tarfile_path),\n",
    "                                                               size = images_tarfile.getmember(fddb_json_annotations[\"file\"]).size)\n",
    "                    annotation_response = self.upload_file_to_minio(bucket_name = self.FDDB_BUCKET_NAME, \n",
    "                                                                    objname = str(Path(fddb_json_annotations[\"file\"]).with_suffix(\".json\")),\n",
    "                                                                    fileobj = io.BytesIO(str(fddb_json_annotations).encode()),\n",
    "                                                                    size = len(str(fddb_json_annotations).encode()))\n",
    "                    print(image_response, annotation_response)\n",
    "                    self.data_count += 1\n",
    "                    \n",
    "                else:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset fddb__dataset (?? GiB) to /home/jovyan/tensorflow_datasets/fddb__dataset/0.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f3052459d44ad5b36a36176514b39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Completed...', max=1, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed109a9a17884effab784746c66502b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Size...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "196c82b1ee5668c7efdba5d669084bd5 2b9838bb8d6e666060820061439c5033\n",
      "d5a60ea3268460489582852dced0033c 0b08cc4fe8dacf26d246bb95ea06f1a0\n",
      "4399258a4b58526e58c1f36bc7e08034 7f41ab7a76b21869ba89dc2dc1c2b99a\n",
      "79bfff0cad523e69b952de4328045178 bcc722aca15d7f50e589afc64c50bb6a\n",
      "d60f086245b23dcbde85cfdea29eeb00 8659c42484a3bd1d849a7c53a2864df1\n"
     ]
    }
   ],
   "source": [
    "my_builder = tfds.builder(convert(FDDB_Dataset.__name__), download_local=False)\n",
    "my_builder.download_and_prepare(\n",
    "    download_dir=os.path.join(os.getcwd(),\"data/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client = Minio(os.environ[\"S3_ENDPOINT\"],\n",
    "                     access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "                     secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "                     secure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2002/07/19/big/img_423.jpg',\n",
       " '2002/07/19/big/img_581.jpg',\n",
       " '2002/07/23/big/img_474.jpg',\n",
       " '2002/07/24/big/img_402.jpg',\n",
       " '2002/07/24/big/img_518.jpg',\n",
       " '2002/07/27/big/img_970.jpg',\n",
       " '2002/07/31/big/img_228.jpg',\n",
       " '2002/08/04/big/img_769.jpg',\n",
       " '2002/08/07/big/img_1316.jpg',\n",
       " '2002/08/11/big/img_591.jpg']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.object_name for i in list(minio_client.list_objects_v2('fddb', recursive=True))][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client.fget_object('fddb', '2002/07/19/big/img_423.jpg', \"./a.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<urllib3.response.HTTPResponse at 0x7fc5c503eeb8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
