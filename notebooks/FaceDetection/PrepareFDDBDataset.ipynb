{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare FDDB Dataset\n",
    "\n",
    "By: Alexander Comerford (alexanderjcomerford@gmail.com)\n",
    "\n",
    "In this notebook we will take advantage of the `tensorflow_datasets` package to create a `tf.data.Dataset` containing image and annotation data from the [fddb dataset](http://vis-www.cs.umass.edu/fddb/) containing multiple thousands of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FDDB from UMass\n",
    "![image](http://vis-www.cs.umass.edu/fddb/umasslogo.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environments and dependencies\n",
    "\n",
    "Below we will import and prepare libraries and configuration via environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AWS_ACCESS_KEY_ID=self2face\n",
      "env: AWS_SECRET_ACCESS_KEY=self2face\n",
      "env: S3_ENDPOINT=minio.default.svc.cluster.local:9000\n",
      "env: AWS_ENDPOINT_URL=http://minio.default.svc.cluster.local:9000\n",
      "env: S3_USE_HTTPS=0\n",
      "env: S3_VERIFY_SSL=0\n",
      "env: BUCKET_NAME=test-data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "import glob\n",
    "import enum\n",
    "import errno\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from minio import Minio\n",
    "from minio.error import ResponseError\n",
    "import tensorflow_datasets.public_api as tfds\n",
    "\n",
    "get_ipython().run_line_magic(\"env\", \"AWS_ACCESS_KEY_ID=self2face\")\n",
    "get_ipython().run_line_magic(\"env\", \"AWS_SECRET_ACCESS_KEY=self2face\")\n",
    "get_ipython().run_line_magic(\"env\", \"S3_ENDPOINT=minio.default.svc.cluster.local:9000\")\n",
    "get_ipython().run_line_magic(\"env\", \"AWS_ENDPOINT_URL=http://minio.default.svc.cluster.local:9000\")\n",
    "get_ipython().run_line_magic(\"env\", \"S3_USE_HTTPS=0\")\n",
    "get_ipython().run_line_magic(\"env\", \"S3_VERIFY_SSL=0\")\n",
    "get_ipython().run_line_magic(\"env\", \"BUCKET_NAME=test-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# SHYM, REMOVE FROM UPDATED DOCKERFILE\n",
    "import sys\n",
    "sys.path.append(\"/home/jovyan/data-vol-1/notebooks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs\n",
    "from .defs.UtilityNotebooks.UtilityFunctions import dotdict, \n",
    "                                                    camel_to_snake, \n",
    "                                                    search_path_by_url,\n",
    "                                                    write_file_to_filepath,\n",
    "                                                    latest_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDDB specific functions\n",
    "\n",
    "In the cells below we will be defining utility functions specific towards this notebook defining string normalization, image object reading, and small wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Turn a string to utf8 striped\n",
    "normalize_string_line = lambda line:str(line.decode('utf-8').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use tarfile object to extract file body from tarfile\n",
    "def get_file_from_tar(filepath, tarpath):\n",
    "    '''Given a tarpath extract a file from tar'''\n",
    "    return tarfile.open(tarpath).extractfile(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform python file object to PIL Image \n",
    "def file_to_image(fileobj):\n",
    "    '''Given a file obj, attempt to create an Image'''\n",
    "    nparr = np.frombuffer(fileobj.read(), np.uint8)\n",
    "    img_np = cv2.cvtColor(cv2.imdecode(nparr, 1), cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(img_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom transformation logic to turn\n",
    "## ellipse coordinates to a four coordinate\n",
    "## definition of a box\n",
    "def image_ellipse_to_box(image, major_axis_radius, minor_axis_radius, angle, center_x, center_y, detection_score):\n",
    "    '''Given a PIL image and ellipse information, return a dict with bounding box information'''\n",
    "    \n",
    "    imagew=image.size[1]\n",
    "    imageh=image.size[0]\n",
    "    box = dotdict(\n",
    "        x=1.0*center_x/imagew,\n",
    "        y=1.0*center_y/imageh,\n",
    "        w=1.0*minor_axis_radius*2/imagew,\n",
    "        h=1.0*major_axis_radius*2/imageh,\n",
    "        category=0\n",
    "    )\n",
    "    \n",
    "    if box.w>0 and box.h>0 and box.x-box.w/2>=0 and\\\n",
    "       box.y-box.h/2>=0 and box.x+box.w/2<=1 and box.y+box.h/2<=1:\n",
    "        return box\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDDBDataset\n",
    "\n",
    "Below we will define a `tfds.core.GeneratorBasedBuilder` which will download the images and labels for the `FDDB` dataset which contains a collection of human faces \"in the wild\" in unknown positions, orientations, and locations. Using this dataset we can train machine learning models to detect faces given generic images.\n",
    "\n",
    "### Example images with added annotation\n",
    "<img src=\"http://vis-www.cs.umass.edu/fddb/samples/2002_08_02_big_img_275.jpg\" data-canonical-src=\"http://vis-www.cs.umass.edu/fddb/samples/2002_08_02_big_img_275.jpg\" width=\"200\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FDDBDataset(tfds.core.GeneratorBasedBuilder):\n",
    "    \"\"\"FDDB dataset\"\"\"\n",
    "    \n",
    "    class FDDB_Parse_State(enum.Enum):\n",
    "        '''States of fddb dataset parsing'''\n",
    "        FILEPATH = 1\n",
    "        NUMFACES = 2\n",
    "        FACELOCATION = 3\n",
    "        \n",
    "    FDDB_BUCKET_NAME = \"fddb\"\n",
    "        \n",
    "    FDDB_DOWNLOAD_URLS={\n",
    "        \"images\":\"http://tamaraberg.com/faceDataset/originalPics.tar.gz\",\n",
    "        \"annotations\":\"http://vis-www.cs.umass.edu/fddb/FDDB-folds.tgz\"\n",
    "    }\n",
    "\n",
    "    VERSION = tfds.core.Version('0.0.0')\n",
    "    \n",
    "    def __init__(self, download_local, download_local_root=\"\", log_every=10,\n",
    "                 *args, **kwargs):\n",
    "        super(FDDBDataset, self).__init__(*args, *kwargs)\n",
    "        self.download_local = download_local\n",
    "        \n",
    "        if download_local: assert not download_local_root, \"Must provide download_local_root path\"\n",
    "        self.download_local_root = download_local_root\n",
    "        \n",
    "        self.minio_client = self.make_minio_client()\n",
    "        self.data_count = 0\n",
    "        self.log_every = log_every\n",
    "\n",
    "    def _info(self):\n",
    "        return tfds.core.DatasetInfo(\n",
    "            builder=self,\n",
    "            features=tfds.features.FeaturesDict({\n",
    "                \"image\": tfds.features.Image(),\n",
    "                \"bbox\": tfds.features.BBoxFeature()\n",
    "            }),\n",
    "            supervised_keys=(\"image\", \"bbox\")\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        try:\n",
    "            dl_paths = dl_manager.download(self.FDDB_DOWNLOAD_URLS)\n",
    "        except tfds.download.download_manager.NonMatchingChecksumError:\n",
    "            pass\n",
    "        \n",
    "        return self.extract_and_upload()\n",
    "        \n",
    "    def _generate_examples(self):\n",
    "        # Yields examples from the dataset\n",
    "        pass  # TODO        \n",
    "\n",
    "    def make_minio_client(self, **kwargs):\n",
    "        return Minio(os.environ[\"S3_ENDPOINT\"],\n",
    "                     access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "                     secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "                     secure=False,\n",
    "                     **kwargs)\n",
    "    \n",
    "    def upload_file_to_minio(self, bucket_name, objname, fileobj, size):\n",
    "        '''Wrapper for uploading a file opject to minio'''\n",
    "        try:\n",
    "            if not self.minio_client.bucket_exists(bucket_name):\n",
    "                self.minio_client.make_bucket(bucket_name)\n",
    "        except ResponseError:\n",
    "            pass        \n",
    "        \n",
    "        return self.minio_client.put_object(bucket_name, objname, fileobj, size)\n",
    "    \n",
    "    def generate_fddb_json_annotations(self, annotations_file_content, images_tarfile, image_file_extension=\".jpg\"):\n",
    "        '''Generator of json annotations for fddb dataset'''\n",
    "\n",
    "        ## Define base dict, state variable, and first line\n",
    "        base_json_annotation = dotdict()\n",
    "        current_state = self.FDDB_Parse_State.FILEPATH\n",
    "        line = normalize_string_line(annotations_file_content.readline())\n",
    "\n",
    "        ## Iter line until empty file\n",
    "        while line:\n",
    "\n",
    "            if current_state == self.FDDB_Parse_State.FILEPATH:\n",
    "                image = file_to_image(images_tarfile.extractfile(line + image_file_extension))\n",
    "                base_json_annotation['file'] = line + image_file_extension\n",
    "                base_json_annotation['imagew'] = image.size[1]\n",
    "                base_json_annotation['imageh'] = image.size[0]            \n",
    "                current_state = self.FDDB_Parse_State.NUMFACES\n",
    "\n",
    "            elif current_state == self.FDDB_Parse_State.NUMFACES:\n",
    "                face_locations = []\n",
    "                num_faces = int(line)\n",
    "                current_state = self.FDDB_Parse_State.FACELOCATION\n",
    "\n",
    "            elif current_state == self.FDDB_Parse_State.FACELOCATION:\n",
    "                if num_faces > 0:\n",
    "                    face_location_args = map(float,line.split())\n",
    "                    bbox = image_ellipse_to_box(image, *face_location_args)\n",
    "                    if bbox: face_locations.append(bbox)\n",
    "                    num_faces -= 1\n",
    "                else:\n",
    "                    if len(face_locations):\n",
    "                        yield dotdict({\n",
    "                            **base_json_annotation, \n",
    "                            **{\"face_locations\":face_locations}})                \n",
    "                    current_state = self.FDDB_Parse_State.FILEPATH\n",
    "                    continue\n",
    "\n",
    "            line = normalize_string_line(annotations_file_content.readline())\n",
    "            \n",
    "    def extract_and_upload(self):\n",
    "\n",
    "        ## Download paths of images and annotations\n",
    "        images_tarfile_path = search_path_by_url(self.FDDB_DOWNLOAD_URLS[\"images\"])\n",
    "        images_tarfile = tarfile.open(images_tarfile_path)\n",
    "        annotations_tarfile_path = search_path_by_url(self.FDDB_DOWNLOAD_URLS[\"annotations\"])\n",
    "        annotations_tarfile = tarfile.open(annotations_tarfile_path)\n",
    "\n",
    "        ## Iterate through 'ellipse' files extracting facial annotations\n",
    "        for annotation_file in filter(lambda tfn:\"ellipse\" in tfn.name, annotations_tarfile):\n",
    "            for fddb_json_annotations in self.generate_fddb_json_annotations(annotations_tarfile.extractfile(annotation_file), images_tarfile):\n",
    "                \n",
    "                image_fileobj = get_file_from_tar(fddb_json_annotations[\"file\"], images_tarfile_path)\n",
    "                image_filepath = fddb_json_annotations[\"file\"]\n",
    "                annotation_fileobj = io.BytesIO(json.dumps(fddb_json_annotations).encode())\n",
    "                annotation_filepath = str(Path(fddb_json_annotations[\"file\"]).with_suffix(\".json\"))\n",
    "                \n",
    "                if not self.download_local:\n",
    "                    image_response = self.upload_file_to_minio(bucket_name = self.FDDB_BUCKET_NAME, \n",
    "                                                               objname = image_filepath,\n",
    "                                                               fileobj = image_fileobj,\n",
    "                                                               size = images_tarfile.getmember(image_filepath).size)\n",
    "                    annotation_response = self.upload_file_to_minio(bucket_name = self.FDDB_BUCKET_NAME, \n",
    "                                                                    objname = annotation_filepath,\n",
    "                                                                    fileobj = annotation_fileobj,\n",
    "                                                                    size = len(str(fddb_json_annotations).encode()))\n",
    "                    \n",
    "                else:\n",
    "                    image_response = write_file_to_filepath(fileobj = image_fileobj,\n",
    "                                                            filepath = os.path.join(self.download_local_root, self.FDDB_BUCKET_NAME, image_filepath))\n",
    "\n",
    "                    annotation_response = write_file_to_filepath(fileobj = annotation_fileobj,\n",
    "                                                                 filepath = os.path.join(self.download_local_root, self.FDDB_BUCKET_NAME, annotation_filepath))\n",
    "                    \n",
    "                    \n",
    "                if self.data_count % self.log_every == 0:\n",
    "                    if self.download_local:\n",
    "                        print(\"Uploaded %d images&annotations to %s\"%(self.data_count,self.download_local_root))\n",
    "                    else:\n",
    "                        print(\"Uploaded %d images&annotations to %s\"%(self.data_count,self.minio_client._endpoint_url))\n",
    "                        \n",
    "                self.data_count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run dataset collection\n",
    "\n",
    "This dataset supports two different modes of operation (local or upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset fddb_dataset (?? GiB) to /home/jovyan/tensorflow_datasets/fddb_dataset/0.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6a9ac24a18452e8f35cd5602d120d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Completed...', max=1, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261750573653461b8dd4142a02831382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Size...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploaded 0 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 10 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 20 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 30 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 40 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 50 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 60 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 70 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 80 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 90 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 100 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 110 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 120 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 130 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 140 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 150 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 160 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 170 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 180 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 190 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 200 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 210 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 220 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 230 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 240 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 250 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 260 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 270 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 280 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 290 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 300 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 310 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 320 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 330 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 340 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 350 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 360 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 370 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 380 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 390 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 400 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 410 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 420 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 430 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 440 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 450 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 460 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 470 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 480 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 490 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 500 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 510 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 520 images&annotations to http://minio.default.svc.cluster.local:9000\n",
      "Uploaded 530 images&annotations to http://minio.default.svc.cluster.local:9000\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "ResponseError: code: XMinioStorageFull, message: Storage backend has reached its minimum free disk threshold. Please delete a few objects to proceed., bucket_name: fddb, object_name: 2002/08/09/big/img_431.jpg, request_id: 15A53BEEB9227509, host_id: 3L137, region: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ab7d437dc391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfddb_dataset_builder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcamel_to_snake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFDDBDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_local\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m fddb_dataset_builder.download_and_prepare(\n\u001b[0;32m----> 3\u001b[0;31m     download_dir=os.path.join(os.getcwd(),\"data/\"))\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/self2face-gpu/lib/python3.6/site-packages/tensorflow_datasets/core/api_utils.py\u001b[0m in \u001b[0;36mdisallow_positional_args_dec\u001b[0;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0m_check_no_positional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mismethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0m_check_required\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdisallow_positional_args_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/self2face-gpu/lib/python3.6/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    248\u001b[0m         self._download_and_prepare(\n\u001b[1;32m    249\u001b[0m             \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             download_config=download_config)\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# NOTE: If modifying the lines below to put additional information in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/self2face-gpu/lib/python3.6/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m    804\u001b[0m     super(GeneratorBasedBuilder, self)._download_and_prepare(\n\u001b[1;32m    805\u001b[0m         \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m     )\n\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/self2face-gpu/lib/python3.6/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;31m# Generating data for all splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[0msplit_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplits_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplitDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msplit_generator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m       \u001b[0;31m# Keep track of all split_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_info_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-a47721420816>\u001b[0m in \u001b[0;36m_split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_and_upload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_generate_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-a47721420816>\u001b[0m in \u001b[0;36mextract_and_upload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m                                                                \u001b[0mobjname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_filepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                                                                \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_fileobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                                                                size = images_tarfile.getmember(image_filepath).size)\n\u001b[0m\u001b[1;32m    131\u001b[0m                     annotation_response = self.upload_file_to_minio(bucket_name = self.FDDB_BUCKET_NAME, \n\u001b[1;32m    132\u001b[0m                                                                     \u001b[0mobjname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotation_filepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-a47721420816>\u001b[0m in \u001b[0;36mupload_file_to_minio\u001b[0;34m(self, bucket_name, objname, fileobj, size)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminio_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_fddb_json_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations_file_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_tarfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_file_extension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/self2face-gpu/lib/python3.6/site-packages/minio/api.py\u001b[0m in \u001b[0;36mput_object\u001b[0;34m(self, bucket_name, object_name, data, length, content_type, metadata, sse, progress, part_size)\u001b[0m\n\u001b[1;32m    836\u001b[0m                                    \u001b[0mcurrent_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m                                    \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                                    progress=progress)\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlist_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/self2face-gpu/lib/python3.6/site-packages/minio/api.py\u001b[0m in \u001b[0;36m_do_put_object\u001b[0;34m(self, bucket_name, object_name, part_data, part_size, upload_id, part_number, metadata, sse, progress)\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m             \u001b[0mcontent_sha256\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msha256_hex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m         )\n\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/self2face-gpu/lib/python3.6/site-packages/minio/api.py\u001b[0m in \u001b[0;36m_url_open\u001b[0;34m(self, method, bucket_name, object_name, query, body, headers, content_sha256, preload_content)\u001b[0m\n\u001b[1;32m   1899\u001b[0m                                     \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m                                     \u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1901\u001b[0;31m                                     object_name).get_exception()\n\u001b[0m\u001b[1;32m   1902\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1903\u001b[0m                 raise ValueError('Unsupported method returned'\n",
      "\u001b[0;31mResponseError\u001b[0m: ResponseError: code: XMinioStorageFull, message: Storage backend has reached its minimum free disk threshold. Please delete a few objects to proceed., bucket_name: fddb, object_name: 2002/08/09/big/img_431.jpg, request_id: 15A53BEEB9227509, host_id: 3L137, region: "
     ]
    }
   ],
   "source": [
    "fddb_dataset_builder = tfds.builder(camel_to_snake(FDDBDataset.__name__), download_local=False)\n",
    "fddb_dataset_builder.download_and_prepare(\n",
    "    download_dir=os.path.join(os.getcwd(),\"data/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidEndpointError",
     "evalue": "InvalidEndpointError: message: Hostname cannot have a scheme.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidEndpointError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-74358c779bff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                      \u001b[0maccess_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"AWS_ACCESS_KEY_ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                      \u001b[0msecret_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"AWS_SECRET_ACCESS_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                      secure=False)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/self2face-gpu/lib/python3.6/site-packages/minio/api.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endpoint, access_key, secret_key, session_token, secure, region, http_client)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# Validate endpoint.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mis_valid_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# Validate http client has correct base class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/self2face-gpu/lib/python3.6/site-packages/minio/helpers.py\u001b[0m in \u001b[0;36mis_valid_endpoint\u001b[0;34m(endpoint)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0murlsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidEndpointError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hostname cannot have a scheme.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidEndpointError\u001b[0m: InvalidEndpointError: message: Hostname cannot have a scheme."
     ]
    }
   ],
   "source": [
    "minio_client = Minio(os.environ[\"MINIO_SERVICE_PORT\"],\n",
    "                     access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "                     secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "                     secure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2002/07/19/big/img_423.jpg',\n",
       " '2002/07/19/big/img_423.json',\n",
       " '2002/07/19/big/img_581.jpg',\n",
       " '2002/07/19/big/img_581.json',\n",
       " '2002/07/23/big/img_474.jpg',\n",
       " '2002/07/24/big/img_402.jpg',\n",
       " '2002/07/24/big/img_402.json',\n",
       " '2002/07/24/big/img_518.jpg',\n",
       " '2002/07/27/big/img_970.jpg',\n",
       " '2002/07/31/big/img_228.jpg']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.object_name for i in list(minio_client.list_objects_v2('fddb', recursive=True))][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client.fget_object('fddb', '2002/07/19/big/img_423.jpg', \"./a.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<urllib3.response.HTTPResponse at 0x7fc5c503eeb8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
