{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "In this notebook we will be generically preparing data... from minio!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_SERVICE='minio.default.svc.cluster.local:9000'\n",
    "MINIO_ACCESS_KEY='self2face'\n",
    "MINIO_SECRET_KEY='self2face'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AWS_ACCESS_KEY_ID=self2face\n",
      "env: AWS_SECRET_ACCESS_KEY=self2face\n",
      "env: S3_ENDPOINT=minio.default.svc.cluster.local:9000\n",
      "env: AWS_ENDPOINT_URL=http://minio.default.svc.cluster.local:9000\n",
      "env: S3_USE_HTTPS=0\n",
      "env: S3_VERIFY_SSL=0\n",
      "env: BUCKET_NAME=test-data\n"
     ]
    }
   ],
   "source": [
    "get_ipython().run_line_magic(\"env\", \"AWS_ACCESS_KEY_ID=self2face\")\n",
    "get_ipython().run_line_magic(\"env\", \"AWS_SECRET_ACCESS_KEY=self2face\")\n",
    "get_ipython().run_line_magic(\"env\", \"S3_ENDPOINT=minio.default.svc.cluster.local:9000\")\n",
    "get_ipython().run_line_magic(\"env\", \"AWS_ENDPOINT_URL=http://minio.default.svc.cluster.local:9000\")\n",
    "get_ipython().run_line_magic(\"env\", \"S3_USE_HTTPS=0\")\n",
    "get_ipython().run_line_magic(\"env\", \"S3_VERIFY_SSL=0\")\n",
    "get_ipython().run_line_magic(\"env\", \"BUCKET_NAME=test-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "import glob\n",
    "import enum\n",
    "import errno\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from minio import Minio\n",
    "from minio.error import ResponseError\n",
    "import tensorflow_datasets.public_api as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# servicename.namespace.svc.cluster.local\n",
    "# minio_client = Minio(os.environ[\"S3_ENDPOINT\"],\n",
    "#                      access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "#                      secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "#                      secure=False)\n",
    "\n",
    "# buckets = minio_client.list_buckets()\n",
    "\n",
    "# for bucket in buckets:\n",
    "#     print(bucket.name, bucket.creation_date)\n",
    "#     print(list(map(get_name, minio_client.list_objects_v2(\"test-data\", '/', recursive=True))))\n",
    "\n",
    "# minio_client.bucket_exists(\"test-data\")\n",
    "\n",
    "# get_name = lambda object: object.object_name\n",
    "# names = map(get_name, client.list_objects_v2(\"test-data\", '/', recursive=True))\n",
    "# for err in client.remove_objects(\"test-data\", names):\n",
    "#     print(\"Deletion Error: {}\".format(err))\n",
    "# client.remove_bucket(\"test-data\")\n",
    "\n",
    "# client.make_bucket(\"test-data\")\n",
    "\n",
    "# client.fput_object('test-data','asdf.jpg','./2002/08/11/big/img_591.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_string_line = lambda line:str(line.decode('utf-8').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_from_tar(filepath, tarpath):\n",
    "    '''Given a tarpath extract a file from tar'''\n",
    "    return tarfile.open(tarpath).extractfile(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_image(fileobj):\n",
    "    '''Given a file obj, attempt to create an Image'''\n",
    "    nparr = np.frombuffer(fileobj.read(), np.uint8)\n",
    "    img_np = cv2.cvtColor(cv2.imdecode(nparr, 1), cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(img_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_dir(input_dir):\n",
    "    return max(glob.glob(os.path.join(input_dir, '*/')), key=os.path.getmtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_download_path(url, base_dir=\"./data\"):\n",
    "    base_download_filename = os.path.basename(url)\n",
    "    for (dirpath, dirnames, filenames) in os.walk(base_dir):\n",
    "        if base_download_filename in filenames:\n",
    "            downloaded_filepath = os.path.join(dirpath, filenames[0])\n",
    "            break\n",
    "    return downloaded_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file_to_filepath(fileobj, filepath):\n",
    "    if not os.path.exists(os.path.dirname(filepath)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filepath))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        f.write(fileobj.read())\n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDDB specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_ellipse_to_box(image, major_axis_radius, minor_axis_radius, angle, center_x, center_y, detection_score):\n",
    "    '''Given a PIL image and ellipse information, return a dict with bounding box information'''\n",
    "    \n",
    "    imagew=image.size[1]\n",
    "    imageh=image.size[0]\n",
    "    box = dotdict(\n",
    "        x=1.0*center_x/imagew,\n",
    "        y=1.0*center_y/imageh,\n",
    "        w=1.0*minor_axis_radius*2/imagew,\n",
    "        h=1.0*major_axis_radius*2/imageh,\n",
    "        category=0\n",
    "    )\n",
    "    \n",
    "    if box.w>0 and box.h>0 and box.x-box.w/2>=0 and\\\n",
    "       box.y-box.h/2>=0 and box.x+box.w/2<=1 and box.y+box.h/2<=1:\n",
    "        return box\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FDDBDataset(tfds.core.GeneratorBasedBuilder):\n",
    "    \"\"\"FDDB dataset\"\"\"\n",
    "    \n",
    "    class FDDB_Parse_State(enum.Enum):\n",
    "        '''States of fddb dataset parsing'''\n",
    "        FILEPATH = 1\n",
    "        NUMFACES = 2\n",
    "        FACELOCATION = 3\n",
    "        \n",
    "    FDDB_BUCKET_NAME = \"fddb\"\n",
    "        \n",
    "    FDDB_DOWNLOAD_URLS={\n",
    "        \"images\":\"http://tamaraberg.com/faceDataset/originalPics.tar.gz\",\n",
    "        \"annotations\":\"http://vis-www.cs.umass.edu/fddb/FDDB-folds.tgz\"\n",
    "    }\n",
    "\n",
    "    VERSION = tfds.core.Version('0.0.0')\n",
    "    \n",
    "    def __init__(self, download_local, download_local_root=\"\", *args, **kwargs):\n",
    "        super(FDDBDataset, self).__init__(*args, *kwargs)\n",
    "        self.download_local = download_local\n",
    "        \n",
    "        if download_local: assert not download_local_root, \"Must provide download_local_root path\"\n",
    "        self.download_local_root = download_local_root\n",
    "        \n",
    "        self.minio_client = self.make_minio_client()\n",
    "        self.data_count = 0\n",
    "\n",
    "    def _info(self):\n",
    "        return tfds.core.DatasetInfo(\n",
    "            builder=self,\n",
    "            features=tfds.features.FeaturesDict({\n",
    "                \"image\": tfds.features.Image(),\n",
    "                \"bbox\": tfds.features.BBoxFeature()\n",
    "            }),\n",
    "            supervised_keys=(\"image\", \"bbox\")\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        try:\n",
    "            dl_paths = dl_manager.download(self.FDDB_DOWNLOAD_URLS)\n",
    "        except tfds.download.download_manager.NonMatchingChecksumError:\n",
    "            pass\n",
    "        \n",
    "        return self.extract_and_upload()\n",
    "        \n",
    "    def _generate_examples(self):\n",
    "        # Yields examples from the dataset\n",
    "        pass  # TODO        \n",
    "\n",
    "    def make_minio_client(self, **kwargs):\n",
    "        return Minio(os.environ[\"S3_ENDPOINT\"],\n",
    "                     access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "                     secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "                     secure=False,\n",
    "                     **kwargs)\n",
    "    \n",
    "    def upload_file_to_minio(self, bucket_name, objname, fileobj, size):\n",
    "        '''Wrapper for uploading a file opject to minio'''\n",
    "        try:\n",
    "            if not self.minio_client.bucket_exists(bucket_name):\n",
    "                self.minio_client.make_bucket(bucket_name)\n",
    "        except ResponseError:\n",
    "            pass        \n",
    "        \n",
    "        return self.minio_client.put_object(bucket_name, objname, fileobj, size)\n",
    "    \n",
    "    def generate_fddb_json_annotations(self, annotations_file_content, images_tarfile, image_file_extension=\".jpg\"):\n",
    "        '''Generator of json annotations for fddb dataset'''\n",
    "\n",
    "        ## Define base dict, state variable, and first line\n",
    "        base_json_annotation = dotdict()\n",
    "        current_state = self.FDDB_Parse_State.FILEPATH\n",
    "        line = normalize_string_line(annotations_file_content.readline())\n",
    "\n",
    "        ## Iter line until empty file\n",
    "        while line:\n",
    "\n",
    "            if current_state == self.FDDB_Parse_State.FILEPATH:\n",
    "                image = file_to_image(images_tarfile.extractfile(line + image_file_extension))\n",
    "                base_json_annotation['file'] = line + image_file_extension\n",
    "                base_json_annotation['imagew'] = image.size[1]\n",
    "                base_json_annotation['imageh'] = image.size[0]            \n",
    "                current_state = self.FDDB_Parse_State.NUMFACES\n",
    "\n",
    "            elif current_state == self.FDDB_Parse_State.NUMFACES:\n",
    "                face_locations = []\n",
    "                num_faces = int(line)\n",
    "                current_state = self.FDDB_Parse_State.FACELOCATION\n",
    "\n",
    "            elif current_state == self.FDDB_Parse_State.FACELOCATION:\n",
    "                if num_faces > 0:\n",
    "                    face_location_args = map(float,line.split())\n",
    "                    bbox = image_ellipse_to_box(image, *face_location_args)\n",
    "                    if bbox: face_locations.append(bbox)\n",
    "                    num_faces -= 1\n",
    "                else:\n",
    "                    if len(face_locations):\n",
    "                        yield dotdict({\n",
    "                            **base_json_annotation, \n",
    "                            **{\"face_locations\":face_locations}})                \n",
    "                    current_state = self.FDDB_Parse_State.FILEPATH\n",
    "                    continue\n",
    "\n",
    "            line = normalize_string_line(annotations_file_content.readline())\n",
    "            \n",
    "    def extract_and_upload(self):\n",
    "\n",
    "        ## Download paths of images and annotations\n",
    "        images_tarfile_path = search_download_path(self.FDDB_DOWNLOAD_URLS[\"images\"])\n",
    "        images_tarfile = tarfile.open(images_tarfile_path)\n",
    "        annotations_tarfile_path = search_download_path(self.FDDB_DOWNLOAD_URLS[\"annotations\"])\n",
    "        annotations_tarfile = tarfile.open(annotations_tarfile_path)\n",
    "\n",
    "        ## Iterate through 'ellipse' files extracting facial annotations\n",
    "        for annotation_file in filter(lambda tfn:\"ellipse\" in tfn.name, annotations_tarfile):\n",
    "            for fddb_json_annotations in self.generate_fddb_json_annotations(annotations_tarfile.extractfile(annotation_file), images_tarfile):\n",
    "                \n",
    "                image_fileobj = get_file_from_tar(fddb_json_annotations[\"file\"], images_tarfile_path)\n",
    "                image_filepath = fddb_json_annotations[\"file\"]\n",
    "                annotation_fileobj = io.BytesIO(json.dumps(fddb_json_annotations).encode())\n",
    "                annotation_filepath = str(Path(fddb_json_annotations[\"file\"]).with_suffix(\".json\"))\n",
    "                \n",
    "                if not self.download_local:\n",
    "                    image_response = self.upload_file_to_minio(bucket_name = self.FDDB_BUCKET_NAME, \n",
    "                                                               objname = image_filepath,\n",
    "                                                               fileobj = image_fileobj,\n",
    "                                                               size = images_tarfile.getmember(image_filepath).size)\n",
    "                    annotation_response = self.upload_file_to_minio(bucket_name = self.FDDB_BUCKET_NAME, \n",
    "                                                                    objname = annotation_filepath,\n",
    "                                                                    fileobj = annotation_fileobj,\n",
    "                                                                    size = len(str(fddb_json_annotations).encode()))\n",
    "                    \n",
    "                else:\n",
    "                    image_response = write_file_to_filepath(fileobj = image_fileobj,\n",
    "                                                            filepath = os.path.join(self.download_local_root, self.FDDB_BUCKET_NAME, image_filepath))\n",
    "\n",
    "                    annotation_response = write_file_to_filepath(fileobj = annotation_fileobj,\n",
    "                                                                 filepath = os.path.join(self.download_local_root, self.FDDB_BUCKET_NAME, annotation_filepath))\n",
    "                    \n",
    "                    \n",
    "                print(image_response, annotation_response)\n",
    "                self.data_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset fddb_dataset (?? GiB) to /home/jovyan/tensorflow_datasets/fddb_dataset/0.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a65554b86d4fcf94769cfe124a1a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Completed...', max=1, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909ca814c91a44f6bef087b4b5af9ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Size...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_builder = tfds.builder(convert(FDDBDataset.__name__), download_local=True)\n",
    "my_builder.download_and_prepare(\n",
    "    download_dir=os.path.join(os.getcwd(),\"data/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client = Minio(os.environ[\"S3_ENDPOINT\"],\n",
    "                     access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "                     secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "                     secure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2002/07/19/big/img_423.jpg',\n",
       " '2002/07/19/big/img_423.json',\n",
       " '2002/07/19/big/img_581.jpg',\n",
       " '2002/07/19/big/img_581.json',\n",
       " '2002/07/23/big/img_474.jpg',\n",
       " '2002/07/24/big/img_402.jpg',\n",
       " '2002/07/24/big/img_402.json',\n",
       " '2002/07/24/big/img_518.jpg',\n",
       " '2002/07/27/big/img_970.jpg',\n",
       " '2002/07/31/big/img_228.jpg']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.object_name for i in list(minio_client.list_objects_v2('fddb', recursive=True))][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client.fget_object('fddb', '2002/07/19/big/img_423.jpg', \"./a.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<urllib3.response.HTTPResponse at 0x7fc5c503eeb8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
