{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self2Face Presentation\n",
    "\n",
    "By: Alexander Comerford (alexanderjcomerford@gmail.com)\n",
    "\n",
    "This notebook is a presentation markdown notebook to describe the contents of the self2face repo\n",
    "\n",
    "## Table of contents:\n",
    "* [Background on GANs](#first-bullet)\n",
    "* [Training GANS](#second-bullet)\n",
    "* [pix2pix](#pix2pix)\n",
    "* [self2face & kubeflow](#self2face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background on GANs\n",
    "\n",
    "GAN - Generative Adversarial Network\n",
    "\n",
    "https://arxiv.org/abs/1406.2661\n",
    "\n",
    "#### Idea\n",
    "\n",
    "* Train a generative model G(z) to generate data with random noise z as input\n",
    "* Adversary is discriminator D(x) trained to distinguish generated and true data\n",
    "* Represent both G(z) and D(x) by multilayer perceptrons for differentiability\n",
    "\n",
    "#### Analogy\n",
    "* Generative: team of counterfeiters, trying to fool police with fake currency\n",
    "* Discriminative: police, trying to detect the counterfeit currency\n",
    "* Competition drives both to improve, until counterfeits are \n",
    "indistinguishable\n",
    "from genuine currency\n",
    "* Now counterfeiters have as a side-effect learned something about real \n",
    "currency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANS are hard to train\n",
    "\n",
    "Without the right hyperparameters, network architecture, and training procedure, there is a high chance that either the generator or discriminator will overpower the other.\n",
    "\n",
    "A common case of this is the situation where the generator is able to find a flaw in the discriminator by repeatedly outputting an image that fits the data distribution the discriminator is looking for, but is nowhere close to being a readable output. The generator has collapsed onto a single point, and therefore we won’t output a variety of digits.\n",
    "\n",
    "There are also cases where the discriminator becomes too powerful and is able to easily make the distinction between real and fake images.\n",
    "\n",
    "The mathematical intuition behind this phenomenon lies in that GANs are typically trained using gradient descent techniques that are designed to find the minimum value of a cost function, rather than to find the Nash equilibrium of a game. When used to seek for a Nash equilibrium, these algorithms may fail to converge. Further research into game theory and stable optimization techniques may result in GANs that are as easy to train as ConvNets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/42f6f5454dda99d8989f9814989efd50fe807ee8/3-Figure1-1.png)\n",
    "\n",
    "--------------\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*M_YipQF_oC6owsU1VVrfhg.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN is based on the zero-sum non-cooperative game. In short, if one wins the other loses. A zero-sum game is also called minimax. Your opponent wants to maximize its actions and your actions are to minimize them. In game theory, the GAN model converges when the discriminator and the generator reach a Nash equilibrium. This is the optimal point for the minimax equation below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/Selection_0004.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. log prob of D predicting that real-world data is genuine\n",
    "\n",
    "2. log prob of D predicting that G’s \n",
    "generated data is not genuine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here we set up our losses and optimizers.\n",
    "\n",
    "    The upside-down capital delta symbol denotse the gradient of the generator\n",
    "    m is the number of samples\n",
    "    Sigma notation tells you to sum up the function evaluated at particular points determined by the little numbers on top and below the big sigma. It is used to add a series of numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://cdn-images-1.medium.com/max/1600/1*V4nu4ThcHFQXmWKxx9lhbA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The gradient ascent expression for the discriminator. The first term corresponds to optimizing the probability that the real data (x) is rated highly. The second term corresponds to optimizing the probability that the generated data G(z) is rated poorly. Notice we apply the gradient to the discriminator, not the generator.\n",
    "\n",
    "Gradient methods generally work better optimizing log⁡p(x) than p(x) because the gradient of log⁡p(x) is generally more well-scaled. That is, it has a size that consistently and helpfully reflects the objective function's geometry, making it easier to select an appropriate step size and get to the optimum in fewer steps. The computer uses a limited digit floating point representation of fractions, multiplying so many probabilities is guaranteed to be very very close to zero. With log, we don't have this issue.\n",
    "\n",
    "The generator is then optimized in order to increase the probability of the generated data being rated highly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://cdn-images-1.medium.com/max/1600/1*njOrO3uNzUH1dZb7-i-dhw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent expression for the generator. The term corresponds to optimizing the probability that the generated data G(z) is rated highly. Notice we apply the gradient to the generator network, not the discriminator.\n",
    "\n",
    "By alternating gradient optimization between the two networks using these expressions on new batches of real and generated data each time, the GAN will slowly converge to producing data that is as realistic as the network is capable of modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of optimality\n",
    "\n",
    "https://arxiv.org/pdf/1406.2661.pdf#page=4&zoom=100,-111,470"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2pix\n",
    "\n",
    "https://arxiv.org/pdf/1611.07004.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://phillipi.github.io/pix2pix/images/teaser_v3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive papers???\n",
    "\n",
    "https://affinelayer.com/pixsrv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self2face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Personal project aimed at creative a workflow to creating you personal virtual avatar\n",
    "* Built on kubeflow + tensorflow + gpus\n",
    "* 10 minutes of video == ~10,000 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kubeflow project is dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. Our goal is not to recreate other services, but to provide a straightforward way to deploy best-of-breed open-source systems for ML to diverse infrastructures. Anywhere you are running Kubernetes, you should be able to run Kubeflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://camo.githubusercontent.com/bd4adfc06b0e349c47f2bae3319544a2e547c796/68747470733a2f2f7777772e6b756265666c6f772e6f72672f696d616765732f6c6f676f2e737667)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://opensource.com/sites/default/files/uploads/kubeflow_ml-model-training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Components of Kubeflow\n",
    "* Logical components that make up Kubeflow\n",
    "* Chainer Training\n",
    "* <b>Hyperparameter Tuning (Katib)</b>\n",
    "* Istio Integration (for TF Serving)\n",
    "* <b>Jupyter Notebooks</b>\n",
    "* ModelDB\n",
    "* ksonnet\n",
    "* MPI Training\n",
    "* MXNet Training\n",
    "* Pipelines\n",
    "* PyTorch Training\n",
    "* Seldon Serving\n",
    "* NVIDIA TensorRT Inference Server\n",
    "* TensorFlow Serving\n",
    "* TensorFlow Batch Predict\n",
    "* TensorFlow Training (TFJob)\n",
    "* PyTorch Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMOOOOO!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
